{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd7d52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this tutorial, I will show all operation required to create cluster and tables.\n",
    "\n",
    "# Make sure install boto3,pg8000,psycopg2 ,pandas using pip\n",
    "# Make sure have access key and secret key \n",
    "\n",
    "# we are creating subnet group,security group ,tables in redshift and also upload data from s3 into redshift cluster\n",
    "#clean the system drop tables ,subnet group and security group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7770e068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:54:42.091302200Z",
     "start_time": "2024-07-21T10:54:42.078334100Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "#import psycopg2\n",
    "\n",
    "access_key = 'AKIA47CR2BBGAXUAHT7V'\n",
    "secret_key = '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'\n",
    "\n",
    "vpc_id='vpc-0c4773efb68c1c9e1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "73eef1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:54:46.549422500Z",
     "start_time": "2024-07-21T10:54:43.236217600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Security group already exists. Using existing security group with ID: sg-0b0ed01ff1798e79e\n"
     ]
    }
   ],
   "source": [
    "#create security group by passing vpc_id and group name\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#create security group\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key,\n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "group_name = 'my-redshift-security-group'\n",
    "group_description = 'Security group for Redshift cluster access'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create the security group\n",
    "    response = ec2_client.create_security_group(\n",
    "        GroupName=group_name,\n",
    "        Description=group_description,\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "    security_group_id = response['GroupId']\n",
    "    print('Created security group with ID:', security_group_id)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'InvalidGroup.Duplicate':\n",
    "        # The security group already exists\n",
    "        response = ec2_client.describe_security_groups(\n",
    "            Filters=[\n",
    "                {'Name': 'group-name', 'Values': [group_name]},\n",
    "                {'Name': 'vpc-id', 'Values': [vpc_id]}\n",
    "            ]\n",
    "        )\n",
    "        security_group_id = response['SecurityGroups'][0]['GroupId']\n",
    "        print('Security group already exists. Using existing security group with ID:', security_group_id)\n",
    "    else:\n",
    "        # Handle other exceptions\n",
    "        print('Error creating security group:', e)\n",
    "\n",
    "\n",
    "#sg-08f82fe187f58b3ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b17c55e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:54:49.555489600Z",
     "start_time": "2024-07-21T10:54:47.923319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg-0b0ed01ff1798e79e\n",
      "Inbound rule already exists for the specified port and IP range.\n"
     ]
    }
   ],
   "source": [
    "#create inbound rule for security group\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "\n",
    "# Create an EC2 client\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key, \n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "#security_group_id sg-08f82fe187f58b3ab  retun by above code\n",
    "print(security_group_id)  \n",
    "\n",
    "\n",
    "port = 5439\n",
    "ip_range = '0.0.0.0/0'\n",
    "\n",
    "try:\n",
    "    # Add the inbound rule to the security group\n",
    "    response = ec2_client.authorize_security_group_ingress(\n",
    "        GroupId=security_group_id,\n",
    "        IpPermissions=[\n",
    "            {\n",
    "                'IpProtocol': 'tcp',\n",
    "                'FromPort': port,\n",
    "                'ToPort': port,\n",
    "                'IpRanges': [{'CidrIp': ip_range}]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print('Inbound rule added to the security group.')\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'InvalidPermission.Duplicate':\n",
    "        print('Inbound rule already exists for the specified port and IP range.')\n",
    "    else:\n",
    "        print('Error adding inbound rule:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a17b2d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:54:53.579603200Z",
     "start_time": "2024-07-21T10:54:52.122982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subnet ID: subnet-0cbc0ceb70af1aa18\n",
      "VPC ID: vpc-0c4773efb68c1c9e1\n",
      "CIDR Block: 172.31.80.0/20\n",
      "Availability Zone: us-east-1a\n",
      "---\n",
      "Subnet ID: subnet-0683e081e785e9fac\n",
      "VPC ID: vpc-0c4773efb68c1c9e1\n",
      "CIDR Block: 172.31.32.0/20\n",
      "Availability Zone: us-east-1c\n",
      "---\n",
      "Subnet ID: subnet-04dc822cbb0f60fed\n",
      "VPC ID: vpc-0c4773efb68c1c9e1\n",
      "CIDR Block: 172.31.64.0/20\n",
      "Availability Zone: us-east-1f\n",
      "---\n",
      "Subnet ID: subnet-044a43ada113c6774\n",
      "VPC ID: vpc-0c4773efb68c1c9e1\n",
      "CIDR Block: 172.31.48.0/20\n",
      "Availability Zone: us-east-1e\n",
      "---\n",
      "Subnet ID: subnet-0ebbec1546112c6b7\n",
      "VPC ID: vpc-0c4773efb68c1c9e1\n",
      "CIDR Block: 172.31.16.0/20\n",
      "Availability Zone: us-east-1b\n",
      "---\n",
      "Subnet ID: subnet-05c209ce933fa4bd0\n",
      "VPC ID: vpc-0c4773efb68c1c9e1\n",
      "CIDR Block: 172.31.0.0/20\n",
      "Availability Zone: us-east-1d\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# check vpc and subnet\n",
    "\n",
    "\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key, \n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "# Fetch and print subnet details\n",
    "response = ec2_client.describe_subnets()\n",
    "\n",
    "for subnet in response['Subnets']:\n",
    "    subnet_id = subnet['SubnetId']\n",
    "    vpc_id = subnet['VpcId']\n",
    "    cidr_block = subnet['CidrBlock']\n",
    "    availability_zone = subnet['AvailabilityZone']\n",
    "    print(f\"Subnet ID: {subnet_id}\")\n",
    "    print(f\"VPC ID: {vpc_id}\")\n",
    "    print(f\"CIDR Block: {cidr_block}\")\n",
    "    print(f\"Availability Zone: {availability_zone}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "44f83ccc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:54:55.359921600Z",
     "start_time": "2024-07-21T10:54:55.346659800Z"
    }
   },
   "outputs": [],
   "source": [
    "# The create_cluster_subnet_group operation in Amazon Redshift is used to create a subnet group that \n",
    "# represents a group of subnets. When creating a Redshift cluster, \n",
    "# you are required to associate the cluster with a subnet group.\n",
    "\n",
    "# Deploying a cluster in multiple subnets allows you to distribute the cluster across \n",
    "# different availability zones (AZs). \n",
    "\n",
    "# If you have data sources or clients in different regions or AZs, placing the cluster in multiple subnets closer \n",
    "# to those data sources or clients can help reduce data transfer costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6dea9cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:55:03.614738200Z",
     "start_time": "2024-07-21T10:55:01.571912400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subnet group already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "#Need to create subnet group which contains list of subnet\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key\n",
    "                              )\n",
    "\n",
    "subnet_group_name = 'my-subnet-group'\n",
    "subnet_ids = ['subnet-0cbc0ceb70af1aa18',\n",
    "              'subnet-0ebbec1546112c6b7',\n",
    "              'subnet-0683e081e785e9fac',\n",
    "              'subnet-05c209ce933fa4bd0',\n",
    "              'subnet-044a43ada113c6774',\n",
    "              'subnet-04dc822cbb0f60fed']   # Replace with the appropriate subnet IDs\n",
    "\n",
    "try:\n",
    "    # Create the subnet group\n",
    "    response = redshift_client.create_cluster_subnet_group(\n",
    "        ClusterSubnetGroupName=subnet_group_name,\n",
    "        Description='My subnet group for redshift description',\n",
    "        SubnetIds=subnet_ids\n",
    "    )\n",
    "    print(subnet_group_name)\n",
    "    print('Subnet group created successfully.')\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ClusterSubnetGroupAlreadyExists':\n",
    "        print('Subnet group already exists. Skipping creation.')\n",
    "    else:\n",
    "        print('Error creating subnet group:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2ff1536d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:55:29.264848900Z",
     "start_time": "2024-07-21T10:55:29.249539600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the cluster parameters used to create cluster in redshift\n",
    "cluster_parameters = {\n",
    "    'ClusterIdentifier': 'my-redshift',\n",
    "    'NodeType': 'dc2.large',\n",
    "    'MasterUsername': 'awsuser',\n",
    "    'MasterUserPassword': 'Google123',\n",
    "    'DBName': 'mydatabase',\n",
    "    'ClusterType': 'single-node',\n",
    "    'NumberOfNodes': 1,\n",
    "    'PubliclyAccessible': True,\n",
    "    'VpcSecurityGroupIds': [security_group_id],  # you take from above we already create security group\n",
    "    'AvailabilityZone': 'us-east-1a', # primarily created in the specified availability zone.\n",
    "    'Port': 5439,\n",
    "    'ClusterSubnetGroupName': 'my-subnet-group'    #created above wih name my-subnet-group\n",
    "     \n",
    "    # Add any other necessary cluster parameters here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ef57aff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:57:37.242294500Z",
     "start_time": "2024-07-21T10:55:31.034781300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift cluster creation initiated.\n",
      "Redshift cluster is now available.\n"
     ]
    }
   ],
   "source": [
    "#finally create cluster in redshift by passing cluster parameter\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "\n",
    "\n",
    "# Create the cluster\n",
    "try:\n",
    "    response = redshift_client.create_cluster(**cluster_parameters)\n",
    "    print('Redshift cluster creation initiated.')\n",
    "except redshift_client.exceptions.ClusterAlreadyExistsFault:\n",
    "    print('Cluster already exists. Skipping cluster creation.')\n",
    "    # You can choose to exit the program or perform other actions as needed\n",
    "    # exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ClusterIdentifier parameter specifies the unique identifier for your Redshift cluster.\n",
    "#The redshift_client.get_waiter('cluster_available').wait() statement waits until the Redshift cluster becomes available. \n",
    "#By default, it will continuously check the cluster status until it becomes available \n",
    "# Wait for the cluster to be available\n",
    "\n",
    "redshift_client.get_waiter('cluster_available').wait(\n",
    "    ClusterIdentifier=cluster_parameters['ClusterIdentifier']\n",
    ")\n",
    "\n",
    "print('Redshift cluster is now available.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b9e75e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T10:57:45.609188800Z",
     "start_time": "2024-07-21T10:57:43.915827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::891377289292:role/redshift-python-test\n"
     ]
    }
   ],
   "source": [
    "#I already created mynewredshiftfortest this role and have permission for s3 access to redshift.\n",
    "\n",
    "iam=boto3.client('iam',\n",
    "                  region_name='us-east-1a',\n",
    "                  aws_access_key_id=access_key,\n",
    "                  aws_secret_access_key=secret_key)\n",
    "\n",
    "roleArn=iam.get_role(RoleName='redshift-python-test')['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e98beb3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:00:15.474389500Z",
     "start_time": "2024-07-21T11:00:13.871827400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These roles will be granted permissions to access s3 within the cluster.\n"
     ]
    }
   ],
   "source": [
    "#modify_cluster_iam_roles method is used to modify the IAM roles associated with an Amazon Redshift cluster.\n",
    "s3_access_role_arn = roleArn\n",
    "\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "redshift_client.modify_cluster_iam_roles(\n",
    "    ClusterIdentifier=cluster_parameters['ClusterIdentifier'],\n",
    "    AddIamRoles=[s3_access_role_arn]\n",
    ")\n",
    "\n",
    "print('These roles will be granted permissions to access s3 within the cluster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "96aa75d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:00:26.394361Z",
     "start_time": "2024-07-21T11:00:24.732297400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ClusterIdentifier': 'my-redshift', 'NodeType': 'dc2.large', 'ClusterStatus': 'available', 'ClusterAvailabilityStatus': 'Available', 'MasterUsername': 'awsuser', 'DBName': 'mydatabase', 'Endpoint': {'Address': 'my-redshift.cza2pgnwtmmi.us-east-1.redshift.amazonaws.com', 'Port': 5439}, 'ClusterCreateTime': datetime.datetime(2024, 7, 21, 10, 56, 48, 829000, tzinfo=tzutc()), 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0b0ed01ff1798e79e', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'my-subnet-group', 'VpcId': 'vpc-0c4773efb68c1c9e1', 'AvailabilityZone': 'us-east-1a', 'PreferredMaintenanceWindow': 'tue:09:00-tue:09:30', 'PendingModifiedValues': {}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 1, 'PubliclyAccessible': True, 'Encrypted': False, 'ClusterPublicKey': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCH9nvmyuYKALnQ1zLB8VAh4o0pao1XFQ1MkiIbdTKOvLCvrLNfGUsA/RG+/xwYNVSAfPdVMa2CWqw9xlGZS8hY0VUUm/GAw/2s4lgZ0BDurteG5vzPFLXBNBddps42dXcyvOamNO87+DJJQ384BZeUDN/KIIklHBVw6FSx1ipVO/4Us61k0TkB9c/AET6xO8UoQLrhDy2b4aJ/udnNjgm3AsH1xk5zxhpMr2G0X0ksyJsvjpXJeKzeGe8xOm+qxmcon3Ynjsrxmkz64C7syORstmq6g/ttRLC0iPhHXD07+s8X1A0SSvPSoLoNDNW32fG6glysblcuRfyTP6MoMTUz Amazon-Redshift\\n', 'ClusterNodes': [{'NodeRole': 'SHARED', 'PrivateIPAddress': '172.31.86.138', 'PublicIPAddress': '18.214.231.231'}], 'ClusterRevisionNumber': '70634', 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::891377289292:role/redshift-python-test', 'ApplyStatus': 'in-sync'}], 'MaintenanceTrackName': 'current', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2024, 7, 23, 9, 0, tzinfo=tzutc()), 'AvailabilityZoneRelocationStatus': 'disabled', 'ClusterNamespaceArn': 'arn:aws:redshift:us-east-1:891377289292:namespace:36848c99-94a8-4004-8b5f-d684411a01df', 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}, 'MultiAZ': 'Disabled'}\n"
     ]
    }
   ],
   "source": [
    "#describe the cluster value\n",
    "# retrieves information about a specific Redshift cluster identified by the ClusterIdentifier my-redshift\n",
    "\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "cluster_info =redshift_client.describe_clusters(ClusterIdentifier='my-redshift')['Clusters'][0]\n",
    "print(cluster_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8344b230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:03:25.770524500Z",
     "start_time": "2024-07-21T11:03:23.100399100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_table table created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "#pg8000 and psycopg2 are both Python libraries used for interacting with PostgreSQL databases, including Amazon Redshift.\n",
    "#pg8000 is a lightweight and pure-Python PostgreSQL adapter that aims for simplicity and ease of use.\n",
    "#psycopg2 is known for its performance and is often the preferred choice for high-performance database interactions.\n",
    "\n",
    "import pg8000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "redshift_endpoint = 'my-redshift.cza2pgnwtmmi.us-east-1.redshift.amazonaws.com'\n",
    "redshift_port = 5439\n",
    "redshift_user = 'awsuser'\n",
    "redshift_password = 'Google123'\n",
    "redshift_database = 'mydatabase'\n",
    "redshift_table = 'product_table'\n",
    "\n",
    "# Create a connection to Redshift using pg8000\n",
    "conn = pg8000.connect(host=redshift_endpoint,\n",
    "                      port=redshift_port,\n",
    "                      database=redshift_database,\n",
    "                      user=redshift_user,\n",
    "                      password=redshift_password)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table if it does not exist\n",
    "create_table_command = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product_table (\n",
    "marketplace varchar(50),\n",
    "customer_id varchar(50),\n",
    "product_id varchar(50),\n",
    "seller_id varchar(50),\n",
    "sell_date varchar(50),\n",
    "quantity integer\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the create table command\n",
    "    cursor.execute(create_table_command)\n",
    "    conn.commit()\n",
    "    print('product_table table created successfully or already exists.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error creating table:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b188d030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:04:00.864976500Z",
     "start_time": "2024-07-21T11:03:57.632755200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "#insert data into table using copy command\n",
    "#copy data from s3 bucket into redshift table\n",
    "#s3://redshift-python-demo/input/product_data.csv  copy data into redshift table product_table\n",
    "\n",
    "\n",
    "import pg8000\n",
    "\n",
    "\n",
    "# Create a connection to Redshift\n",
    "#execute a copy command in cluster . to copy data from s3://mypythonproject1/input/product_data.csv into redshift table\n",
    "conn = pg8000.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL statements\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "input_bucket = 'redshift-python-demo'\n",
    "input_file_key = 'input/product_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "copy_command = f\"\"\"\n",
    "COPY public.product_table\n",
    "FROM 's3://redshift-python-demo/input/product_data.csv'\n",
    "CREDENTIALS 'aws_access_key_id={access_key};aws_secret_access_key={secret_key}'\n",
    "DELIMITER ',' IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the COPY command\n",
    "    cursor.execute(copy_command)\n",
    "    conn.commit()\n",
    "    print('COPY command executed successfully.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error executing COPY command:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef01258a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:17:09.703436600Z",
     "start_time": "2024-07-21T11:17:07.503338700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "#psycopg2 is known for its performance and is often the preferred choice for high-performance database interactions.\n",
    "#create table emp in redshift using psycopg2\n",
    "\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "redshift_endpoint = 'my-redshift.cza2pgnwtmmi.us-east-1.redshift.amazonaws.com'\n",
    "redshift_port = 5439\n",
    "redshift_user = 'awsuser'\n",
    "redshift_password = 'Google123'\n",
    "redshift_database = 'mydatabase'\n",
    "redshift_table = 'product_table'\n",
    "\n",
    "# Create a connection to Redshift using pg8000\n",
    "conn = psycopg2.connect(host=redshift_endpoint,\n",
    "                      port=redshift_port,\n",
    "                      database=redshift_database,\n",
    "                      user=redshift_user,\n",
    "                      password=redshift_password)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table if it does not exist\n",
    "create_table_command = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS emp (\n",
    "emp_id int,\n",
    "name varchar(100),\n",
    "salary decimal\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the create table command\n",
    "    cursor.execute(create_table_command)\n",
    "    conn.commit()\n",
    "    print('emp created successfully or already exists.')\n",
    "except psycopg2.Error as e:\n",
    "    print('Error creating table:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8836c73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:17:14.228286100Z",
     "start_time": "2024-07-21T11:17:12.160529Z"
    }
   },
   "outputs": [
    {
     "ename": "InternalError_",
     "evalue": "Load into table 'emp' failed.  Check 'stl_load_errors' system table for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInternalError_\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[117], line 36\u001B[0m\n\u001B[0;32m     26\u001B[0m copy_command \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124mCOPY public.emp\u001B[39m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;124mFROM \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms3://redshift-python-demo/input/emp.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;124mCREDENTIALS \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maws_access_key_id=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccess_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m;aws_secret_access_key=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msecret_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;124mDELIMITER \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m IGNOREHEADER 1;\u001B[39m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;66;03m# Execute the COPY command\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m     \u001B[43mcursor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcopy_command\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     conn\u001B[38;5;241m.\u001B[39mcommit()\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCOPY command executed successfully.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mInternalError_\u001B[0m: Load into table 'emp' failed.  Check 'stl_load_errors' system table for details.\n"
     ]
    }
   ],
   "source": [
    "#copy venue_pip txt file into table emp in redshift using psycopg2\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# Create a connection to Redshift\n",
    "#execute a copy command in cluster . to copy data from s3://mypythonproject1/input/product_data.csv into redshift table\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL statements\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "input_bucket = 'redshift-python-demo'\n",
    "input_file_key = 'input/emp.txt'\n",
    "\n",
    "\n",
    "\n",
    "copy_command = f\"\"\"\n",
    "COPY public.emp\n",
    "FROM 's3://redshift-python-demo/input/emp.txt'\n",
    "CREDENTIALS 'aws_access_key_id={access_key};aws_secret_access_key={secret_key}'\n",
    "DELIMITER ',' IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the COPY command\n",
    "    cursor.execute(copy_command)\n",
    "    conn.commit()\n",
    "    print('COPY command executed successfully.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error executing COPY command:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print('COPY command executed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "51678283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:17:30.886849700Z",
     "start_time": "2024-07-21T11:17:28.410513800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 107046, datetime.datetime(2024, 7, 21, 11, 11, 45, 450139), 1073848595, 1877, 's3://redshift-python-demo/input/emp.txt                                                                                                                                                                                                                         ', 2, 'emp_id                                                                                                                         ', 'int4      ', '0         ', 0, '123|John Smith|5000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ', '123|John Smith|5000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ', 1214, 'Delimiter not found                                                                                 ', 0, 0, 0)\n",
      "(100, 0, 107056, datetime.datetime(2024, 7, 21, 11, 17, 13, 601291), 1073963396, 2006, 's3://redshift-python-demo/input/emp.txt                                                                                                                                                                                                                         ', 2, 'emp_id                                                                                                                         ', 'int4      ', '0         ', 0, '123|John Smith|5000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ', '123|John Smith|5000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ', 1214, 'Delimiter not found                                                                                 ', 0, 0, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   userid  slice     tbl                  starttime     session  query  \\\n0     100      1  107046 2024-07-21 11:11:45.450139  1073848595   1877   \n1     100      0  107056 2024-07-21 11:17:13.601291  1073963396   2006   \n\n                                            filename  line_number  \\\n0  s3://redshift-python-demo/input/emp.txt       ...            2   \n1  s3://redshift-python-demo/input/emp.txt       ...            2   \n\n                                             colname        type  col_length  \\\n0  emp_id                                        ...  int4        0            \n1  emp_id                                        ...  int4        0            \n\n   position                                           raw_line  \\\n0         0  123|John Smith|5000                           ...   \n1         0  123|John Smith|5000                           ...   \n\n                                     raw_field_value  err_code  \\\n0  123|John Smith|5000                           ...      1214   \n1  123|John Smith|5000                           ...      1214   \n\n                                          err_reason  is_partial  \\\n0  Delimiter not found                           ...           0   \n1  Delimiter not found                           ...           0   \n\n   start_offset  copy_job_id  \n0             0            0  \n1             0            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userid</th>\n      <th>slice</th>\n      <th>tbl</th>\n      <th>starttime</th>\n      <th>session</th>\n      <th>query</th>\n      <th>filename</th>\n      <th>line_number</th>\n      <th>colname</th>\n      <th>type</th>\n      <th>col_length</th>\n      <th>position</th>\n      <th>raw_line</th>\n      <th>raw_field_value</th>\n      <th>err_code</th>\n      <th>err_reason</th>\n      <th>is_partial</th>\n      <th>start_offset</th>\n      <th>copy_job_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1</td>\n      <td>107046</td>\n      <td>2024-07-21 11:11:45.450139</td>\n      <td>1073848595</td>\n      <td>1877</td>\n      <td>s3://redshift-python-demo/input/emp.txt       ...</td>\n      <td>2</td>\n      <td>emp_id                                        ...</td>\n      <td>int4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>123|John Smith|5000                           ...</td>\n      <td>123|John Smith|5000                           ...</td>\n      <td>1214</td>\n      <td>Delimiter not found                           ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100</td>\n      <td>0</td>\n      <td>107056</td>\n      <td>2024-07-21 11:17:13.601291</td>\n      <td>1073963396</td>\n      <td>2006</td>\n      <td>s3://redshift-python-demo/input/emp.txt       ...</td>\n      <td>2</td>\n      <td>emp_id                                        ...</td>\n      <td>int4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>123|John Smith|5000                           ...</td>\n      <td>123|John Smith|5000                           ...</td>\n      <td>1214</td>\n      <td>Delimiter not found                           ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check error\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL query to retrieve load error details from 'stl_load_errors'\n",
    "cur.execute(\"SELECT * FROM stl_load_errors\")\n",
    "\n",
    "# Fetch all the rows returned by the query\n",
    "load_errors = cur.fetchall()\n",
    "\n",
    "#Print the load error details\n",
    "for error in load_errors:\n",
    "    print(error)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "#format is not good \n",
    "\n",
    "\n",
    "# Create a dataframe from the load_errors data so convert into dataframe\n",
    "df = pd.DataFrame(load_errors, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "# Display the dataframe\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b2aa7fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:19:22.363199700Z",
     "start_time": "2024-07-21T11:19:19.955610500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "#Now correct DELIMITER and again execute code\n",
    "#we need to use pipe delemiter\n",
    "\n",
    "#copy emp txt file into table emp in redshift using psycopg2\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# Create a connection to Redshift\n",
    "#execute a copy command in cluster . to copy data from s3://mypythonproject1/input/product_data.csv into redshift table\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL statements\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "input_bucket = 'redshift-python'\n",
    "input_file_key = 'input/emp.txt'\n",
    "\n",
    "\n",
    "\n",
    "copy_command = f\"\"\"\n",
    "COPY public.emp\n",
    "FROM 's3://redshift-python-demo/input/emp.txt'\n",
    "CREDENTIALS 'aws_access_key_id={access_key};aws_secret_access_key={secret_key}'\n",
    "DELIMITER '|' IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the COPY command\n",
    "    cursor.execute(copy_command)\n",
    "    conn.commit()\n",
    "    print('COPY command executed successfully.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error executing COPY command:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "86730c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:19:45.544505900Z",
     "start_time": "2024-07-21T11:19:39.204535900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 'John Smith', Decimal('5000'))\n",
      "(456, 'Jane Doe', Decimal('7500'))\n",
      "(789, 'Alan Johnson', Decimal('10000'))\n"
     ]
    }
   ],
   "source": [
    "#now select data of emp and product from redshift \n",
    "#query into emp and product_table in redshift\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SELECT query\n",
    "cur.execute(\"SELECT * FROM emp\")\n",
    "\n",
    "# Fetch all the rows returned by the query\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Process the retrieved rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a85aceab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:20:20.981123300Z",
     "start_time": "2024-07-21T11:20:14.684613700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('US', '49033728', 'A6302503213', '1111', '31-08-2021', 10)\n",
      "('US', '17857748', 'B000059PET1', '2222', '20-09-2021', 20)\n",
      "('US', '25551507', 'S7888128071', '3333', '31-08-2021', 10)\n",
      "('US', '21025041', 'W630250993', '4444', '20-09-2021', 20)\n",
      "('US', '40943563', 'B00JENS2BI', '5555', '31-08-2021', 10)\n",
      "('US', '17013969', 'J6305761302', '6666', '05-09-2021', 30)\n",
      "('US', '47611685', 'K6300157555', '7777', '06-09-2021', 30)\n",
      "('US', '35680737', 'H6300189570', 'xxxx', '07-09-2021', 40)\n",
      "('US', '10747909', 'B000SXQ5US', 'yyyy', '08-09-2021', 20)\n"
     ]
    }
   ],
   "source": [
    "#now select data of emp and product from redshift \n",
    "#query into emp and product_table in redshift\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SELECT query\n",
    "cur.execute(\"SELECT * FROM product_table\")\n",
    "\n",
    "# Fetch all the rows returned by the query\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Process the retrieved rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f214d824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:21:29.580964500Z",
     "start_time": "2024-07-21T11:21:27.176016200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Now drop Multiple table at once\n",
    "#drop table emp and product_table in redshift\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# List of tables to drop\n",
    "tables_to_drop = ['emp', 'product_table']\n",
    "\n",
    "# Drop the tables\n",
    "for table_name in tables_to_drop:\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "print('Drop table successfully.')    \n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "00a0f8ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:23:45.144220500Z",
     "start_time": "2024-07-21T11:21:39.402601800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete cluster successfully\n"
     ]
    }
   ],
   "source": [
    "#once you done delete your redshift cluster\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "cluster_identifier = 'my-redshift'\n",
    "\n",
    "# Delete the Redshift cluster\n",
    "redshift_client.delete_cluster(ClusterIdentifier=cluster_identifier,\n",
    "                              SkipFinalClusterSnapshot=True)\n",
    "\n",
    "\n",
    "redshift_client.get_waiter('cluster_deleted').wait(ClusterIdentifier=cluster_identifier)\n",
    "\n",
    "print(\"delete cluster successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d6033f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:23:51.785163300Z",
     "start_time": "2024-07-21T11:23:50.178342900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'ResponseMetadata': {'RequestId': 'aa49e914-ac47-4fad-87ef-5c88d5b3b12e',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'aa49e914-ac47-4fad-87ef-5c88d5b3b12e',\n   'content-type': 'text/xml',\n   'content-length': '232',\n   'date': 'Sun, 21 Jul 2024 11:23:50 GMT'},\n  'RetryAttempts': 0}}"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete the subnet group 'my-subnet-group'  which you created above:\n",
    "#subnet_group_name = 'my-subnet-group'\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Delete the subnet group\n",
    "redshift_client.delete_cluster_subnet_group(ClusterSubnetGroupName=subnet_group_name)\n",
    "\n",
    "\n",
    "#A status code of 200 indicates that the request was successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0de5f669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T11:23:56.370400800Z",
     "start_time": "2024-07-21T11:23:54.462424800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'ResponseMetadata': {'RequestId': '5a84f633-adec-47b2-8a0d-193f22657c85',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '5a84f633-adec-47b2-8a0d-193f22657c85',\n   'cache-control': 'no-cache, no-store',\n   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n   'content-type': 'text/xml;charset=UTF-8',\n   'content-length': '283',\n   'date': 'Sun, 21 Jul 2024 11:23:55 GMT',\n   'server': 'AmazonEC2'},\n  'RetryAttempts': 0}}"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete security group  security_group_id  which you created above\n",
    "\n",
    "\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key, \n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "# Delete the security group\n",
    "ec2_client.delete_security_group(GroupId=security_group_id)\n",
    "\n",
    "\n",
    "#'HTTPStatusCode' field in the response. If the value is 200, it indicates a successful \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab9a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
